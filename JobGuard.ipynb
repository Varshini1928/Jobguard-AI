{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da3a3Y3_nJq2",
        "outputId": "77ae0a27-041e-48a6-cd95-d6cf9a9f9927"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Cleaning completed and saved to cleaned_jobs.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Training CNN model...\n",
            "Epoch 1/3\n",
            "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 25ms/step - accuracy: 0.9661 - loss: 0.1956\n",
            "Epoch 2/3\n",
            "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 24ms/step - accuracy: 0.9819 - loss: 0.0647\n",
            "Epoch 3/3\n",
            "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 24ms/step - accuracy: 0.9871 - loss: 0.0376\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ CNN model saved as cnn_model.h5\n",
            "\n",
            " Training RNN (LSTM) model...\n",
            "Epoch 1/3\n",
            "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 114ms/step - accuracy: 0.9565 - loss: 0.2057\n",
            "Epoch 2/3\n",
            "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 114ms/step - accuracy: 0.9679 - loss: 0.1563\n",
            "Epoch 3/3\n",
            "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 112ms/step - accuracy: 0.9652 - loss: 0.1619\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " RNN model saved as rnn_model.h5\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import spacy\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Download stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "# Load SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=['ner', 'parser'])\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. CLEANING FUNCTION\n",
        "# ---------------------------------------------------------\n",
        "def clean_text(text):\n",
        "    text = str(text)\n",
        "\n",
        "    # Remove URLs & websites\n",
        "    text = re.sub(r'http\\S+|www\\S+', ' ', text)\n",
        "\n",
        "    # Remove emails\n",
        "    text = re.sub(r'\\S+@\\S+', ' ', text)\n",
        "\n",
        "    # Remove phone numbers\n",
        "    text = re.sub(r'\\+?\\d[\\d -]{8,12}\\d', ' ', text)\n",
        "\n",
        "    # Remove emojis\n",
        "    text = re.sub(r'[\\U00010000-\\U0010ffff]', '', text)\n",
        "\n",
        "    # Keep only alphabets, numbers, spaces\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. LOAD AND CLEAN DATASET\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "df = pd.read_csv(\"fake_job_posts.csv\")\n",
        "\n",
        "# Keep only important columns\n",
        "columns_to_keep = [\"title\", \"location\", \"description\", \"requirements\", \"salary_range\"]\n",
        "df = df[columns_to_keep]\n",
        "\n",
        "# Remove rows with missing description\n",
        "df = df.dropna(subset=[\"description\"])\n",
        "\n",
        "# Remove duplicate rows\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# Remove very short descriptions\n",
        "df = df[df[\"description\"].str.len() > 20]\n",
        "\n",
        "# Remove non-alphabetic rows\n",
        "df = df[df[\"description\"].str.contains('[A-Za-z]', regex=True)]\n",
        "\n",
        "# Apply cleaning\n",
        "df[\"clean_text\"] = df[\"description\"].apply(clean_text)\n",
        "\n",
        "# Save cleaned file\n",
        "df.to_csv(\"cleaned_jobs.csv\", index=False)\n",
        "print(\"✔ Cleaning completed and saved to cleaned_jobs.csv\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. PREPROCESSING — Lemmatization\n",
        "# ---------------------------------------------------------\n",
        "def lemmatize(text):\n",
        "    doc = nlp(text)\n",
        "    return \" \".join([token.lemma_ for token in doc if token.text not in stopwords])\n",
        "\n",
        "df[\"processed_text\"] = df[\"clean_text\"].apply(lemmatize)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4. TOKENIZATION + PADDING\n",
        "# ---------------------------------------------------------\n",
        "texts = df[\"processed_text\"].tolist()\n",
        "\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "padded = pad_sequences(sequences, maxlen=200, padding='post')\n",
        "\n",
        "# Fake label (0 or 1) for model demo\n",
        "df[\"fake\"] = df[\"title\"].apply(lambda x: 1 if \"intern\" in str(x).lower() else 0)\n",
        "labels = df[\"fake\"].values\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 5. CNN MODEL USING ADAM OPTIMIZER\n",
        "# ---------------------------------------------------------\n",
        "cnn_model = Sequential([\n",
        "    Embedding(10000, 64, input_length=200),\n",
        "    Conv1D(64, 3, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "cnn_model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"\\n Training CNN model...\")\n",
        "cnn_model.fit(padded, labels, epochs=3, batch_size=32)\n",
        "\n",
        "cnn_model.save(\"cnn_model.h5\")\n",
        "print(\"✔ CNN model saved as cnn_model.h5\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 6. RNN (LSTM) MODEL USING ADAM OPTIMIZER\n",
        "# ---------------------------------------------------------\n",
        "rnn_model = Sequential([\n",
        "    Embedding(10000, 64, input_length=200),\n",
        "    LSTM(64, return_sequences=False),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "rnn_model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"\\n Training RNN (LSTM) model...\")\n",
        "rnn_model.fit(padded, labels, epochs=3, batch_size=32)\n",
        "\n",
        "rnn_model.save(\"rnn_model.h5\")\n",
        "print(\" RNN model saved as rnn_model.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# 1. LOAD CLEANED DATASET\n",
        "# -------------------------------------------------------\n",
        "df = pd.read_csv(\"cleaned_jobs.csv\")\n",
        "\n",
        "print(\"Dataset columns:\", list(df.columns))\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# 2. SIMPLE PREPROCESSING FUNCTION\n",
        "#    (use existing clean_text column if available)\n",
        "# -------------------------------------------------------\n",
        "def preprocess_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "if \"clean_text\" in df.columns:\n",
        "    text_col = \"clean_text\"\n",
        "else:\n",
        "    text_col = \"description\"\n",
        "\n",
        "print(f\"Processing column: '{text_col}'\")\n",
        "df[\"processed_text\"] = df[text_col].astype(str).apply(preprocess_text)\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# 3. TF-IDF VECTORISATION\n",
        "# -------------------------------------------------------\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X = tfidf.fit_transform(df[\"processed_text\"])\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# 4. SAVE OUTPUTS\n",
        "# -------------------------------------------------------\n",
        "pd.DataFrame(X.toarray(),\n",
        "             columns=tfidf.get_feature_names_out()).to_csv(\"tfidf_features.csv\",\n",
        "                                                          index=False)\n",
        "df.to_csv(\"final_preprocessed_dataset.csv\", index=False)\n",
        "\n",
        "print(\"Preprocessing Completed!\")\n",
        "print(\"Saved: final_preprocessed_dataset.csv\")\n",
        "print(\"TF-IDF Features Saved: tfidf_features.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keTqR3P-izKi",
        "outputId": "3f7e049e-ec37-4b07-a2b9-643e35feb1c7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset columns: ['title', 'location', 'description', 'requirements', 'salary_range', 'clean_text']\n",
            "Processing column: 'clean_text'\n",
            "Preprocessing Completed!\n",
            "Saved: final_preprocessed_dataset.csv\n",
            "TF-IDF Features Saved: tfidf_features.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Preprocessed text column from your cleaned dataset\n",
        "texts = df[\"processed_text\"]\n",
        "\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=5000,\n",
        "    stop_words=\"english\",\n",
        "    ngram_range=(1,2)   # unigrams + bigrams\n",
        ")\n",
        "\n",
        "X = tfidf.fit_transform(texts)\n",
        "\n",
        "print(\"TF-IDF Shape:\", X.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rL8-CcCWb_1R",
        "outputId": "8bee349b-99a6-4f8a-df80-327976ec4b47"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Shape: (17454, 5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "# -----------------------------\n",
        "# 1. INPUT: Load Dataset\n",
        "# -----------------------------\n",
        "df = pd.read_csv(\"fake_job_posts.csv\")\n",
        "\n",
        "# Keep only important columns\n",
        "df = df[['description', 'fraudulent']]\n",
        "df = df.dropna()\n",
        "\n",
        "# -----------------------------\n",
        "# 2. PREPROCESSING\n",
        "# -----------------------------\n",
        "def clean_text(text):\n",
        "    text = text.lower()                                    # lowercase\n",
        "    text = re.sub(r'http\\S+|www.\\S+', '', text)            # remove URLs\n",
        "    text = re.sub(r'[^a-zA-Z ]', ' ', text)                # remove special chars\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()               # remove extra spaces\n",
        "    return text\n",
        "\n",
        "df['clean_text'] = df['description'].apply(clean_text)\n",
        "\n",
        "# -----------------------------\n",
        "# 3. FEATURE EXTRACTION (CountVectorizer)\n",
        "# -----------------------------\n",
        "vectorizer = CountVectorizer(stop_words='english', max_features=50)\n",
        "X = vectorizer.fit_transform(df['clean_text'])\n",
        "\n",
        "# Get top 50 common words overall\n",
        "common_words = vectorizer.get_feature_names_out()\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# ===============    ANALYSIS TASKS    ====================\n",
        "# =========================================================\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# TASK 1: Visualize fake vs real job posts\n",
        "# ---------------------------------------------------------\n",
        "plt.figure(figsize=(6,4))\n",
        "df['fraudulent'].value_counts().plot(kind='bar')\n",
        "plt.title(\"Fake vs Real Job Posts\")\n",
        "plt.xticks([0,1], labels=['Real (0)', 'Fake (1)'])\n",
        "plt.xlabel(\"Category\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# TASK 2: Text Length Analysis\n",
        "# ---------------------------------------------------------\n",
        "df['text_length'] = df['clean_text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "df['text_length'].hist(bins=40)\n",
        "plt.title(\"Distribution of Text Lengths\")\n",
        "plt.xlabel(\"Number of Words\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ANALYSIS: Compare text length of real vs fake\n",
        "plt.figure(figsize=(7,4))\n",
        "df[df['fraudulent']==0]['text_length'].hist(alpha=0.5, label='Real', bins=40)\n",
        "df[df['fraudulent']==1]['text_length'].hist(alpha=0.5, label='Fake', bins=40)\n",
        "plt.legend()\n",
        "plt.title(\"Text Length Comparison: Real vs Fake\")\n",
        "plt.xlabel(\"Number of Words\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# TASK 3: Common words in FAKE job posts\n",
        "# ---------------------------------------------------------\n",
        "fake_df = df[df['fraudulent'] == 1]\n",
        "\n",
        "fake_vectorizer = CountVectorizer(stop_words='english', max_features=20)\n",
        "fake_matrix = fake_vectorizer.fit_transform(fake_df['clean_text'])\n",
        "\n",
        "fake_words = fake_vectorizer.get_feature_names_out()\n",
        "fake_counts = fake_matrix.sum(axis=0).A1\n",
        "\n",
        "# Plot common words in fake job posts\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.bar(fake_words, fake_counts)\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Top Common Words in Fake Job Posts\")\n",
        "plt.xlabel(\"Words\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LXulP7hKcLYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pickle\n",
        "\n",
        "# --------------------------------------------\n",
        "# 1. LOAD PREPROCESSED DATA\n",
        "# --------------------------------------------\n",
        "df = pd.read_csv(\"final_preprocessed_dataset.csv\")\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "\n",
        "# We already have processed_text\n",
        "TEXT_COL = \"processed_text\"\n",
        "\n",
        "# --------------------------------------------\n",
        "# 2. CREATE LABEL COLUMN (DUMMY EXAMPLE)\n",
        "#    0 = Real, 1 = Fake\n",
        "# --------------------------------------------\n",
        "if \"fraudulent\" not in df.columns:\n",
        "    df[\"fraudulent\"] = 0\n",
        "    # mark first 100 rows as fake (you can change this rule later)\n",
        "    n_fake = min(100, len(df))\n",
        "    df.loc[:n_fake-1, \"fraudulent\"] = 1\n",
        "    print(f\"Created 'fraudulent' label: {n_fake} fake, {len(df)-n_fake} real\")\n",
        "\n",
        "LABEL_COL = \"fraudulent\"\n",
        "\n",
        "X_text = df[TEXT_COL].astype(str)\n",
        "y = df[LABEL_COL]\n",
        "\n",
        "# --------------------------------------------\n",
        "# 3. FEATURE EXTRACTION (TF-IDF)\n",
        "# --------------------------------------------\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X = tfidf.fit_transform(X_text)\n",
        "\n",
        "# Save TF-IDF vectorizer for Flask\n",
        "with open(\"tfidf.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tfidf, f)\n",
        "\n",
        "# --------------------------------------------\n",
        "# 4. TRAIN-TEST SPLIT\n",
        "# --------------------------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# --------------------------------------------\n",
        "# 5. TRAIN MODELS\n",
        "# --------------------------------------------\n",
        "log_model = LogisticRegression(max_iter=1000)\n",
        "log_model.fit(X_train, y_train)\n",
        "\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# --------------------------------------------\n",
        "# 6. PREDICTIONS & METRICS\n",
        "# --------------------------------------------\n",
        "log_pred = log_model.predict(X_test)\n",
        "nb_pred = nb_model.predict(X_test)\n",
        "\n",
        "log_acc = accuracy_score(y_test, log_pred)\n",
        "nb_acc  = accuracy_score(y_test, nb_pred)\n",
        "\n",
        "print(\"\\nLogistic Regression Accuracy :\", log_acc)\n",
        "print(\"Naive Bayes Accuracy        :\", nb_acc)\n",
        "\n",
        "print(\"\\n--- Logistic Regression Report ---\")\n",
        "print(classification_report(y_test, log_pred))\n",
        "\n",
        "print(\"\\n--- Naive Bayes Report ---\")\n",
        "print(classification_report(y_test, nb_pred))\n",
        "\n",
        "# --------------------------------------------\n",
        "# 7. SAVE MODELS FOR FLASK API\n",
        "# --------------------------------------------\n",
        "with open(\"log_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(log_model, f)\n",
        "\n",
        "with open(\"nb_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(nb_model, f)\n",
        "\n",
        "print(\"\\nModels saved as:\")\n",
        "print(\" → tfidf.pkl\")\n",
        "print(\" → log_model.pkl\")\n",
        "print(\" → nb_model.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtH7h-kfkgE7",
        "outputId": "e47e9215-f32f-4322-d359-9db3737661f7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns: ['title', 'location', 'description', 'requirements', 'salary_range', 'clean_text', 'processed_text']\n",
            "Created 'fraudulent' label: 100 fake, 17354 real\n",
            "\n",
            "Logistic Regression Accuracy : 0.9942709825264967\n",
            "Naive Bayes Accuracy        : 0.9939845316528215\n",
            "\n",
            "--- Logistic Regression Report ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      1.00      3471\n",
            "           1       0.00      0.00      0.00        20\n",
            "\n",
            "    accuracy                           0.99      3491\n",
            "   macro avg       0.50      0.50      0.50      3491\n",
            "weighted avg       0.99      0.99      0.99      3491\n",
            "\n",
            "\n",
            "--- Naive Bayes Report ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      1.00      3471\n",
            "           1       0.00      0.00      0.00        20\n",
            "\n",
            "    accuracy                           0.99      3491\n",
            "   macro avg       0.50      0.50      0.50      3491\n",
            "weighted avg       0.99      0.99      0.99      3491\n",
            "\n",
            "\n",
            "Models saved as:\n",
            " → tfidf.pkl\n",
            " → log_model.pkl\n",
            " → nb_model.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  DistilBERT Fake Job Detector - YOUR UPLOADED DATASET (<18 mins)\n",
        "# Perfect for your JobCheck project - Uses YOUR local dataset!\n",
        "\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "import pandas as pd, torch, time, warnings, numpy as np\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "from transformers import DataCollatorWithPadding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "from datasets import Dataset\n",
        "\n",
        "print(\" YOUR DATASET - DistilBERT Fake Job Detector!\")\n",
        "print(\" Analyzing your uploaded dataset...\")\n",
        "start_total = time.time()\n",
        "\n",
        "# 1. LOAD YOUR DATASET (CSV/Excel support)\n",
        "print(\"\\n Loading your dataset...\")\n",
        "# Try common filenames first\n",
        "dataset_files = ['fake_job_postings.csv', 'dataset.csv', 'jobs.csv', 'data.csv', 'train.csv']\n",
        "\n",
        "df = None\n",
        "for filename in dataset_files:\n",
        "    try:\n",
        "        if filename.endswith('.csv'):\n",
        "            df = pd.read_csv(filename)\n",
        "        else:\n",
        "            df = pd.read_excel(filename)\n",
        "        print(f\" Loaded: {filename}\")\n",
        "        break\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "# If no common filename, load from user specification\n",
        "if df is None:\n",
        "    print(\"Available files:\", os.listdir('.'))\n",
        "    # Default to first CSV found\n",
        "    csv_files = [f for f in os.listdir('.') if f.endswith('.csv')]\n",
        "    if csv_files:\n",
        "        df = pd.read_csv(csv_files[0])\n",
        "        print(f\" Auto-loaded: {csv_files[0]}\")\n",
        "\n",
        "print(f\" Dataset shape: {df.shape}\")\n",
        "print(f\" Columns: {list(df.columns)}\")\n",
        "print(\"\\n First 3 rows:\")\n",
        "print(df.head(3))\n",
        "\n",
        "# 2. AUTO-PREPROCESSING (Smart column detection)\n",
        "print(\"\\n Auto-detecting job text & labels...\")\n",
        "\n",
        "# Common text columns\n",
        "text_cols = ['title', 'description', 'job_description', 'text', 'job_title', 'content']\n",
        "text_col = None\n",
        "for col in text_cols:\n",
        "    if col in df.columns:\n",
        "        text_col = col\n",
        "        break\n",
        "\n",
        "if text_col is None:\n",
        "    # Combine title + description if both exist\n",
        "    if 'title' in df.columns and 'description' in df.columns:\n",
        "        df['text'] = df['title'].fillna('') + ' [SEP] ' + df['description'].fillna('')\n",
        "        text_col = 'text'\n",
        "    else:\n",
        "        print(\" No text columns found! Using first string column.\")\n",
        "        str_cols = df.select_dtypes(include=['object']).columns\n",
        "        text_col = str_cols[0]\n",
        "\n",
        "print(f\" Text column: {text_col}\")\n",
        "\n",
        "# Common label columns (0/1, real/fake, legitimate/fraud)\n",
        "label_cols = ['label', 'fake', 'is_fake', 'fraudulent', 'telecommuting', 'has_company_logo']\n",
        "label_col = None\n",
        "\n",
        "for col in label_cols:\n",
        "    if col in df.columns:\n",
        "        label_col = col\n",
        "        break\n",
        "\n",
        "if label_col is None:\n",
        "    # Auto-detect binary column\n",
        "    binary_cols = [col for col in df.columns if df[col].nunique() == 2]\n",
        "    if binary_cols:\n",
        "        label_col = binary_cols[0]\n",
        "        print(f\" Auto-detected label: {label_col}\")\n",
        "\n",
        "# Smart label mapping (0=real, 1=fake)\n",
        "if label_col:\n",
        "    print(f\" Label distribution before processing: {df[label_col].value_counts().to_dict()}\")\n",
        "\n",
        "    # Map common patterns\n",
        "    if df[label_col].dtype == 'object':\n",
        "        df[label_col] = df[label_col].map({'real': 0, 'fake': 1, 'legitimate': 0, 'fraudulent': 1, 0: 0, 1: 1}).fillna(1)\n",
        "    df['label'] = pd.to_numeric(df[label_col], errors='coerce').fillna(1).astype(int)\n",
        "\n",
        "    print(f\" Final labels: Real={sum(df.label==0)}, Fake={sum(df.label==1)}\")\n",
        "\n",
        "# Clean & balance dataset\n",
        "df = df.dropna(subset=[text_col, 'label']).reset_index(drop=True)\n",
        "df['text'] = df[text_col].astype(str)\n",
        "\n",
        "# Balance classes (minority class size)\n",
        "min_class = min(sum(df.label==0), sum(df.label==1))\n",
        "real_sample = df[df.label==0].sample(n=min_class, random_state=42)\n",
        "fake_sample = df[df.label==1].sample(n=min_class, random_state=42)\n",
        "df = pd.concat([real_sample, fake_sample]).reset_index(drop=True)\n",
        "\n",
        "print(f\" Balanced dataset: {len(df):,} jobs (Real: {sum(df.label==0):,}, Fake: {sum(df.label==1):,})\")\n",
        "\n",
        "# 3. TOKENIZATION\n",
        "print(\"\\n Tokenizing your dataset...\")\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "def tokenize(examples):\n",
        "    return tokenizer(examples['text'], truncation=True, padding=True, max_length=256)\n",
        "\n",
        "train_df, test_df = train_test_split(df[['text', 'label']], test_size=0.2, stratify=df.label, random_state=42)\n",
        "train_ds = Dataset.from_pandas(train_df).map(tokenize, batched=True)\n",
        "test_ds = Dataset.from_pandas(test_df).map(tokenize, batched=True)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# 4. TRAIN YOUR MODEL\n",
        "print(\"\\n Training on YOUR dataset...\")\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=TrainingArguments(\n",
        "        output_dir='./your_jobcheck_model',\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=16,\n",
        "        warmup_steps=50,\n",
        "        weight_decay=0.01,\n",
        "        logging_steps=10,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        dataloader_num_workers=0,\n",
        "        report_to=None,\n",
        "        fp16=torch.cuda.is_available(),\n",
        "        save_total_limit=2,\n",
        "        gradient_accumulation_steps=2\n",
        "    ),\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=test_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# 5. RESULTS\n",
        "results = trainer.evaluate()\n",
        "print(f\"\\n YOUR DATASET RESULTS:\")\n",
        "print(f\" Accuracy:  {results['eval_accuracy']*100:.1f}%\")\n",
        "print(f\" F1-Score:  {results['eval_f1']:.3f}\")\n",
        "print(f\" Precision: {results['eval_precision']:.3f}\")\n",
        "print(f\" Recall:    {results['eval_recall']:.3f}\")\n",
        "\n",
        "trainer.save_model(\"./your_jobcheck_model\")\n",
        "tokenizer.save_pretrained(\"./your_jobcheck_model\")\n",
        "print(\" Saved: ./your_jobcheck_model/\")\n",
        "\n",
        "# 6. PRODUCTION PREDICTOR\n",
        "print(\"\\n Test your model:\")\n",
        "def predict_job(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=256)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "    pred = probs.argmax().item()\n",
        "    return \" FAKE\" if pred == 1 else \" REAL\", probs[0][pred].item(), probs[0][1].item()\n",
        "\n",
        "# Interactive testing\n",
        "print(\"\\n Enter job postings to test (Ctrl+C to stop):\")\n",
        "while True:\n",
        "    try:\n",
        "        job_text = input(\"\\nJob posting: \")\n",
        "        if not job_text.strip(): break\n",
        "        verdict, conf, fake_prob = predict_job(job_text)\n",
        "        print(f\"   {verdict} ({conf:.1%} conf) | Fake risk: {fake_prob:.1%}\")\n",
        "    except KeyboardInterrupt:\n",
        "        break\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nS6a-o0kxBiV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}